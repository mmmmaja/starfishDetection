{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Starfish Detection","text":""},{"location":"#project-layout","title":"Project layout","text":"<pre><code>\u251c\u2500\u2500 .dvc/                      # DVC configuration\n\u2502   \u251c\u2500\u2500 config                 # Configuration file for DVC\n\u2502   \u2514\u2500\u2500 .gitignore             # Ignore DVC files in Git\n\u251c\u2500\u2500 .github/                   # GitHub workflows and automation\n\u2502   \u251c\u2500\u2500 workflows/             # CI/CD workflows\n\u2502   \u2502   \u251c\u2500\u2500 cml_data.yaml      # CML (Continuous Machine Learning) workflow\n\u2502   \u2502   \u251c\u2500\u2500 pre_commit.yaml    # Pre-commit checks\n\u2502   \u2502   \u251c\u2500\u2500 stage_model.yaml   # Workflow to stage and deploy models\n\u2502   \u2502   \u2514\u2500\u2500 tests.yaml         # Workflow for running tests\n\u2502   \u2514\u2500\u2500 dependabot.yaml        # Dependabot configuration for dependency updates\n\u251c\u2500\u2500 configs/                   # Configuration files\n\u2502   \u251c\u2500\u2500 callbacks/             # Callback configurations\n\u2502   \u2502   \u251c\u2500\u2500 default_callbacks.yaml\n\u2502   \u2502   \u2514\u2500\u2500 wandb_image_logger.yaml\n\u2502   \u251c\u2500\u2500 experiment/            # Experiment configurations\n\u2502   \u2502   \u251c\u2500\u2500 profile.yaml\n\u2502   \u2502   \u2514\u2500\u2500 train_local.yaml\n\u2502   \u251c\u2500\u2500 logger/                # Logging configurations\n\u2502   \u2502   \u2514\u2500\u2500 wandb_logger.yaml\n\u2502   \u251c\u2500\u2500 model/                 # Model configurations\n\u2502   \u2502   \u2514\u2500\u2500 default_model.yaml\n\u2502   \u251c\u2500\u2500 trainer/               # Trainer configurations\n\u2502   \u2502   \u2514\u2500\u2500 default_trainer.yaml\n\u2502   \u251c\u2500\u2500 starfish_data.yaml     # Dataset configurations\n\u2502   \u251c\u2500\u2500 sweep_config.yaml      # Hyperparameter sweep configurations\n\u2502   \u251c\u2500\u2500 main_config.yaml       # Main configuration file\n\u2502   \u2514\u2500\u2500 vertex_ai_config.yaml  # Vertex AI configuration\n\u251c\u2500\u2500 dockerfiles/               # Dockerfiles for deployment and training\n\u2502   \u251c\u2500\u2500 inference_backend.dockerfile\n\u2502   \u251c\u2500\u2500 inference_frontend.dockerfile\n\u2502   \u251c\u2500\u2500 train.dockerfile\n\u2502   \u2514\u2500\u2500 data_drift.dockerfile  # Dockerfile for data drift detection\n\u251c\u2500\u2500 docs/                      # Project documentation\n\u2502   \u251c\u2500\u2500 figures/               # Documentation-related figures\n\u2502   \u2502   \u251c\u2500\u2500 app-image1.png\n\u2502   \u2502   \u251c\u2500\u2500 app-image2.png\n\u2502   \u2502   \u251c\u2500\u2500 app-image3.png\n\u2502   \u2502   \u251c\u2500\u2500 app-image4.png\n\u2502   \u2502   \u251c\u2500\u2500 app-image5.png\n\u2502   \u2502   \u251c\u2500\u2500 app-image6.png\n\u2502   \u251c\u2500\u2500 README.md              # Docs README file\n\u2502   \u251c\u2500\u2500 application.md         # Documentation about the application\n\u2502   \u251c\u2500\u2500 index.md               # Main page of the documentation\n\u2502   \u2514\u2500\u2500 code.md                # Code-related documentation\n\u251c\u2500\u2500 reports/                   # Reports and visualizations\n\u2502   \u251c\u2500\u2500 figures/               # Result figures\n\u2502   \u2502   \u251c\u2500\u2500 image_logging.png\n\u2502   \u2502   \u251c\u2500\u2500 loss_logging.png\n\u2502   \u2502   \u2514\u2500\u2500 sweep.png\n\u2502   \u251c\u2500\u2500 README.md              # Explanation of reports\n\u2502   \u2514\u2500\u2500 report.py              # Report generation script\n\u251c\u2500\u2500 src/                       # Source code\n\u2502   \u251c\u2500\u2500 starfish/              # Main project package\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py        # Module initialization\n\u2502   \u2502   \u251c\u2500\u2500 apis/              # API-related files\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 inference_backend.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 inference_frontend.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 data_drift.py  # Data drift detection utilities\n\u2502   \u2502   \u251c\u2500\u2500 callbacks.py       # Callbacks for training\n\u2502   \u2502   \u251c\u2500\u2500 data.py            # Data processing utilities\n\u2502   \u2502   \u251c\u2500\u2500 evaluate.py        # Model evaluation\n\u2502   \u2502   \u251c\u2500\u2500 model.py           # Model definitions\n\u2502   \u2502   \u251c\u2500\u2500 onnx_model.py      # ONNX model conversion and inference\n\u2502   \u2502   \u251c\u2500\u2500 profile_forward_pass.py # Profiling scripts\n\u2502   \u2502   \u251c\u2500\u2500 train.py           # Training script\n\u2502   \u2502   \u2514\u2500\u2500 visualize.py       # Visualization utilities\n\u251c\u2500\u2500 tests/                     # Test suite\n\u2502   \u251c\u2500\u2500 integrationtests/      # Integration tests\n\u2502   \u2502   \u2514\u2500\u2500 test_api.py        # API endpoint tests\n\u2502   \u251c\u2500\u2500 performancetests/      # Performance tests\n\u2502   \u2502   \u2514\u2500\u2500 locustfile.py      # Load testing with Locust\n\u2502   \u2514\u2500\u2500 unittests/             # Unit tests\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 test_data.py       # Data-related tests\n\u2502       \u2514\u2500\u2500 test_model.py      # Model-related tests\n\u251c\u2500\u2500 .dvcignore                 # Ignore patterns for DVC\n\u251c\u2500\u2500 .gcloudignore              # Ignore patterns for Google Cloud\n\u251c\u2500\u2500 .gitignore                 # Ignore patterns for Git\n\u251c\u2500\u2500 .pre-commit-config.yaml    # Pre-commit hooks configuration\n\u251c\u2500\u2500 LICENSE                    # License file\n\u251c\u2500\u2500 README.md                  # Project README\n\u251c\u2500\u2500 cloudbuild.yaml            # Cloud Build configuration\n\u251c\u2500\u2500 data.dvc                   # DVC tracking file\n\u251c\u2500\u2500 mkdocs.yml                 # MkDocs configuration\n\u251c\u2500\u2500 pyproject.toml             # Python project metadata\n\u251c\u2500\u2500 requirements.txt           # Python dependencies\n\u251c\u2500\u2500 requirements_dev.txt       # Development dependencies\n\u251c\u2500\u2500 tasks.py                   # Task runner (e.g., Invoke or similar tools)\n\u251c\u2500\u2500 vertex_ai_train.yaml       # Vertex AI training configuration\n</code></pre>"},{"location":"application/","title":"Application","text":"<p>Users can interact with our application using the backend or the frontend.</p>"},{"location":"application/#backend","title":"Backend","text":"<p>To obtain predictions on an image by communicating with the API directly, you can use a curl command. To use the PyTorch model for inference, run</p> <pre><code>curl -X 'POST' 'https://backend-638730968773.us-central1.run.app/inference/' -H 'accept: application/json' -H 'Content-Type: multipart/form-data' -F 'data=@PATH_TO_IMAGE;type=image/jpeg'\n</code></pre> <p>To use the ONNX model for inference, run</p> <pre><code>curl -X 'POST' 'https://backend-638730968773.us-central1.run.app/onnx-inference/' -H 'accept: application/json' -H 'Content-Type: multipart/form-data' -F 'data=@PATH_TO_IMAGE;type=image/jpeg'\n</code></pre>"},{"location":"application/#frontend","title":"Frontend","text":"<p>For a more user-friendly experience, you can access our frontend webpage at https://frontend-638730968773.us-central1.run.app.</p> <p>Follow these steps to get predictions on an image.</p> <ol> <li> <p>Click <code>Browse files</code>. </p> </li> <li> <p>Select your desired image. </p> </li> <li> <p>You should then see the image you selected appear. </p> </li> <li> <p>Once the inference has finished, you will see boxes on your image around predicted starfish along with confidence scores. </p> </li> <li> <p>Below you will also see a histogram over the confidence scores for your image. </p> </li> <li> <p>Finally, you get a matrix of the bounding box coordinates in pixel coordinates. </p> </li> </ol>"},{"location":"code/","title":"Training","text":""},{"location":"code/#environment","title":"Environment","text":"<p>Create a dedicated environment to keep track of the packages for the project</p> <pre><code>conda create --name starfish-env python=3.11\nconda activate starfish-env\npip install -r requirements.txt\npip install -r requirements_dev.txt\npip install -e .\n</code></pre>"},{"location":"code/#data","title":"Data","text":"<p>Download the data for the project from our Google Cloud Bucket, which requires the Google Cloud SDK</p> <pre><code>invoke download-data\n</code></pre>"},{"location":"code/#train","title":"Train","text":"<p>Train with default arguments</p> <pre><code>train\n</code></pre> <p>Train with data downloaded from the bucket rather than accessing it directly from the cloud</p> <pre><code>train data.data_from_bucket=false\n</code></pre>"},{"location":"code/#profiling","title":"Profiling","text":"<p>Train with profiling</p> <pre><code>train profiling=True\n</code></pre> <p>Profile forward pass</p> <pre><code>invoke profile-forward-pass\n</code></pre>"},{"location":"code/#docker","title":"Docker","text":"<p>Build the training dockerfile into a Docker image</p> <pre><code>invoke build-train-image\n</code></pre> <p>Run a container spawned from the docker image</p> <pre><code>invoke run-train-image\n</code></pre> <p>Build an image for the backend and push it to the cloud</p> <pre><code>invoke backend-image-to-cloud\n</code></pre> <p>Build an image for the frontend and push it to the cloud</p> <pre><code>invoke frontend-image-to-cloud\n</code></pre> <p>Test a docker image locally</p> <pre><code>docker run --rm -p 8080:8080 -e \"PORT=8080\" IMAGE_NAME\n</code></pre> <p>Deploy the backend</p> <pre><code>invoke deploy-backend\n</code></pre> <p>Deploy the frontend</p> <pre><code>invoke deploy-frontend\n</code></pre>"},{"location":"code/#vertex-ai","title":"Vertex AI","text":"<p>Get the Docker image built from <code>train.dockerfile</code> in the Artifact Registry on Google Cloud, for example by creating a trigger and using the <code>cloudbuild.yaml</code> file. Then you can train a model using that Docker image through the Vertex AI service. This also automatically logs the training to Wandb if your API key has been stored as a secret in Google Cloud.</p> <pre><code>invoke train-vertex\n</code></pre>"},{"location":"code/#wandb","title":"Wandb","text":"<p>Login</p> <pre><code>wandb login\n</code></pre> <p>Hyperparameter sweep</p> <pre><code>invoke sweep\nwandb agent ENTITY/PROJECT_NAME/AGENT_ID\n</code></pre>"},{"location":"code/#deploy-application","title":"Deploy application","text":"<p>Deploy the application by running the backend and frontend Docker images in the Artifact Registry generated by the Cloud Build trigger and <code>cloudbuild.yaml</code>.</p> <pre><code>gcloud run deploy backend --image=us-central1-docker.pkg.dev/starfish-detection/frontend-backend/backend:latest --region=us-central1 --platform=managed --allow-unauthenticated --port=8080\ngcloud run deploy frontend --image=us-central1-docker.pkg.dev/starfish-detection/frontend-backend/frontend:latest --region=us-central1 --platform=managed --allow-unauthenticated --port=8080\n</code></pre>"},{"location":"code/#tests","title":"Tests","text":"<p>Run all tests and calculate coverage</p> <pre><code>invoke test\n</code></pre> <p>Run data tests</p> <pre><code>invoke test-data\n</code></pre> <p>Run model tests</p> <pre><code>invoke test-model\n</code></pre> <p>Load test</p> <pre><code>locust -f tests/performancetests/locustfile.py --headless --users 10 --spawn-rate 1 --run-time 1m --host https://backend-638730968773.us-central1.run.app\n</code></pre>"},{"location":"code/#documentation-site","title":"Documentation Site","text":"<p>Build site locally</p> <pre><code>mkdocs build\n</code></pre> <p>Deploy site to <code>gh-pages</code> branch</p> <pre><code>mkdocs gh-deploy\n</code></pre>"}]}